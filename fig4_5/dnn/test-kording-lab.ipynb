{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4202befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: statsmodels is not installed. You will be unable to use the Naive Bayes Decoder\n",
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "# If you would prefer to load the '.h5' example file rather than the '.pickle' example file. You need the deepdish package\n",
    "# import deepdish as dd \n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import DenseNNDecoder\n",
    "\n",
    "\n",
    "#Import hyperparameter optimization packages\n",
    "#If either are not installed, give a warning\n",
    "try:\n",
    "    from bayes_opt import BayesianOptimization\n",
    "except ImportError:\n",
    "    print(\"\\nWARNING: BayesianOptimization package is not installed. You will be unable to use section 4.\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1bba8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load Data###\n",
    "folder = 'input_data' #ENTER THE FOLDER THAT YOUR DATA IS IN\n",
    "anm = 'JEB7'\n",
    "date = '2021-04-30'\n",
    "\n",
    "data=io.loadmat(folder + '/' + anm + '_' + date + '.mat')\n",
    "\n",
    "N = data['gpfadat'] # (time,trials,factors) of gpfa latent trajectories\n",
    "M = data['kindat']  # (time,trials,features) of kinematic features reduced with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50fde1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 254, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nTrials = N.shape[1]\n",
    "\n",
    "idx = np.arange(0,nTrials)\n",
    "\n",
    "randidx = np.random.choice(idx, size=(nTrials,1), replace=False).flatten()\n",
    "\n",
    "\n",
    "\n",
    "# # shuffle trials\n",
    "N = N[:,list(randidx),:]\n",
    "M = M[:,list(randidx),:]\n",
    "\n",
    "N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "db884fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape N and M to be (time*trials,factors/features)\n",
    "N_reshape = N.reshape((N.shape[0]*N.shape[1],N.shape[2]))\n",
    "M_reshape = M.reshape((M.shape[0]*M.shape[1],M.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa9c085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_before = 20  # How many bins of neural data prior to the output are used for decoding\n",
    "bins_current = 1  # Whether to use concurrent time bin of neural data\n",
    "bins_after = 0    # How many bins of neural data after the output are used for decoding\n",
    "\n",
    "\n",
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X = get_spikes_with_history(N_reshape,bins_before,bins_after,bins_current) # (time*trials,nbins,factors)\n",
    "\n",
    "# Format for  Dense Neural Network\n",
    "# Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "X_flat = X.reshape(X.shape[0],(X.shape[1]*X.shape[2])) # (time*trials, nbins*factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35559d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set decoding output\n",
    "y = M_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b630b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set what part of data should be part of the training/testing/validation sets\n",
    "training_range = [0, 0.7]\n",
    "testing_range = [0.7, 0.85]\n",
    "valid_range = [0.85,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa9ab548",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples=X.shape[0]\n",
    "\n",
    "# Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "# This makes it so that the different sets don't include overlapping neural data\n",
    "training_set = np.arange(int(np.round(training_range[0]*num_examples))+bins_before,int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "testing_set = np.arange(int(np.round(testing_range[0]*num_examples))+bins_before,int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "valid_set = np.arange(int(np.round(valid_range[0]*num_examples))+bins_before,int(np.round(valid_range[1]*num_examples))-bins_after)\n",
    "\n",
    "# Get training data\n",
    "X_train = X[training_set,:,:]\n",
    "X_flat_train = X_flat[training_set,:]\n",
    "y_train = y[training_set,:]\n",
    "\n",
    "# Get testing data\n",
    "X_test = X[testing_set,:,:]\n",
    "X_flat_test = X_flat[testing_set,:]\n",
    "y_test = y[testing_set,:]\n",
    "\n",
    "# Get validation data\n",
    "X_valid = X[valid_set,:,:]\n",
    "X_flat_valid = X_flat[valid_set,:]\n",
    "y_valid = y[valid_set,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ca56580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score \"X\" inputs. \n",
    "X_train_mean = np.nanmean(X_train,axis=0)\n",
    "X_train_std = np.nanstd(X_train,axis=0)\n",
    "X_train = (X_train-X_train_mean)/X_train_std\n",
    "X_test = (X_test-X_train_mean)/X_train_std\n",
    "X_valid = (X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "# Z-score \"X_flat\" inputs. \n",
    "X_flat_train_mean = np.nanmean(X_flat_train,axis=0)\n",
    "X_flat_train_std = np.nanstd(X_flat_train,axis=0)\n",
    "\n",
    "X_flat_train = (X_flat_train-X_flat_train_mean)/X_flat_train_std \n",
    "X_flat_test = (X_flat_test-X_flat_train_mean)/X_flat_train_std \n",
    "X_flat_valid = (X_flat_valid-X_flat_train_mean)/X_flat_train_std \n",
    "\n",
    "#Zero-center outputs\n",
    "y_train_mean = np.mean(y_train,axis=0)\n",
    "\n",
    "y_train = y_train-y_train_mean \n",
    "y_test =  y_test-y_train_mean \n",
    "y_valid =  y_valid-y_train_mean \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b9e9c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dnn_evaluate(units,dropout):\n",
    "    model = DenseNNDecoder(units=units,dropout=dropout,num_epochs=20) #Define model\n",
    "    model.fit(X_flat_train,y_train) #Fit model\n",
    "    y_valid_predicted=model.predict(X_flat_valid) #Validation set predictions\n",
    "    return np.mean(get_R2(y_valid,y_valid_predicted)) #R2 value of validation set (mean over x and y position/velocity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7616f999",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/bayes_opt/target_space.py:191\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_hashable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.39027142544993554, 340.54295940585746)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m dnnBO \u001b[38;5;241m=\u001b[39m BayesianOptimization(dnn_evaluate, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m399.99\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.99\u001b[39m)}, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#kappa is a parameter that sets exploration vs exploitation in the algorithm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#We set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdnnBO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkappa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:185\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    182\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    183\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:116\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/bayes_opt/target_space.py:194\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 194\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mdnn_evaluate\u001b[0;34m(units, dropout)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdnn_evaluate\u001b[39m(units,dropout):\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m DenseNNDecoder(units\u001b[38;5;241m=\u001b[39munits,dropout\u001b[38;5;241m=\u001b[39mdropout,num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m) \u001b[38;5;66;03m#Define model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_flat_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Fit model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     y_valid_predicted\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_flat_valid) \u001b[38;5;66;03m#Validation set predictions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(get_R2(y_valid,y_valid_predicted))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/Neural_Decoding/decoders.py:396\u001b[0m, in \u001b[0;36mDenseNNRegression.fit\u001b[0;34m(self, X_flat_train, y_train)\u001b[0m\n\u001b[1;32m    394\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_flat_train,y_train,nb_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose) \u001b[38;5;66;03m#Fit the model\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 396\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_flat_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Fit the model\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m=\u001b[39mmodel\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2955\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2952\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m-> 2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mgraph_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3262\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3258\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3259\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments supplied to `defun`-generated functions must be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3260\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhashable.  Original error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3262\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3263\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mUSE_FUNCTION_SUBTYPING\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/function_cache.py:160\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, key, use_function_subtyping)\u001b[0m\n\u001b[1;32m    157\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary:\n\u001b[0;32m--> 160\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_primary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache:\n\u001b[1;32m    163\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[key]]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/neural-decoding/lib/python3.9/site-packages/tensorflow/python/eager/function_cache.py:106\u001b[0m, in \u001b[0;36mFunctionCacheKey.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, FunctionCacheKey):\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_context \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39mcall_context \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_signature\u001b[49m \u001b[38;5;241m==\u001b[39m other\u001b[38;5;241m.\u001b[39mfunction_signature)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Define Bayesian optimization, and set limits of hyperparameters \n",
    "#Here, we set the limit of \"degree\" to be [1, 6.99], so we test degrees 1,2,3,4,5,6\n",
    "dnnBO = BayesianOptimization(dnn_evaluate, {'units': (10, 399.99), 'dropout': (0,0.99)}, verbose=0)\n",
    "#Set number of initial runs (init_points) and subsequent tests (n_iter), and do the optimization\n",
    "#kappa is a parameter that sets exploration vs exploitation in the algorithm\n",
    "#We set kappa=10 (greater than the default) so there is more exploration when there are more hyperparameters\n",
    "dnnBO.maximize(init_points=5, n_iter=5, kappa=10) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69d487de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dropout = dnnBO.max['params']['dropout']\n",
    "best_units = dnnBO.max['params']['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50de79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e9ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748e567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4746fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec05cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1cf7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48fc4883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2s: [-2.93062595e+02 -1.03886342e-01  3.72477907e-01 -1.58794681e-01\n",
      " -5.92395645e-02 -6.09336424e-02 -7.45069124e-02]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Declare model\n",
    "# model_dnn=DenseNNDecoder(units=best_units,dropout=best_dropout,num_epochs=1000)\n",
    "model_dnn=DenseNNDecoder(units=[400],dropout=0.1,num_epochs=20)\n",
    "\n",
    "#Fit model\n",
    "model_dnn.fit(X_flat_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_dnn=model_dnn.predict(X_flat_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2s_dnn=get_R2(y_valid,y_valid_predicted_dnn)\n",
    "print('R2s:', R2s_dnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cfc94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457946e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcae6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c93604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23e70ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d8cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978df6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d7e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c551bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d72e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed768c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
